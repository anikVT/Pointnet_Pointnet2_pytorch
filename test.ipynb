{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from models.pointnet2_cls_msg import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# This should be the directory that contains your module, directly specified\n",
    "package_directory = '/home/hani/GitHub/PCR_practice/Old_notebooks'\n",
    "\n",
    "# Add the directory to sys.path\n",
    "if package_directory not in sys.path:\n",
    "    sys.path.append(package_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now you can import your module as if it were in the same director\n",
    "from simdata_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_pnpp_original  = 'log/classification/pointnet2_msg_normals/checkpoints/best_model.pth'\n",
    "#checkpoint_path_pnpp_mdnetc = '/home/hani/GitHub/PCR_practice/Old_notebooks/models/mdnet_40_C_pnpp_da/model_best_test.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_pnpp_original = torch.load(checkpoint_path_pnpp_original, map_location=torch.device('cpu'))\n",
    "#checkpoint_pnpp_mdnetc = torch.load(checkpoint_path_pnpp_mdnetc, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'instance_acc', 'class_acc', 'model_state_dict', 'optimizer_state_dict'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_pnpp_original.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the state dict of the model\n",
    "\n",
    "model_state_dict_pnpp_original = checkpoint_pnpp_original['model_state_dict']\n",
    "#model_state_dict_pnpp_mdnetc = checkpoint_pnpp_mdnetc['model_state']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load architecture of the model original\n",
    "model_pnpp_original  = get_model(40,normal_channel=True)\n",
    "model_pnpp_original.load_state_dict(model_state_dict_pnpp_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_model(\n",
       "  (sa1): PointNetSetAbstractionMsg(\n",
       "    (conv_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Conv2d(6, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (bn_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sa2): PointNetSetAbstractionMsg(\n",
       "    (conv_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Conv2d(323, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Conv2d(323, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Conv2d(323, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (bn_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sa3): PointNetSetAbstraction(\n",
       "    (mlp_convs): ModuleList(\n",
       "      (0): Conv2d(643, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (mlp_bns): ModuleList(\n",
       "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop1): Dropout(p=0.4, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### set the model to evaluation mode\n",
    "model_pnpp_original.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances 1300\n"
     ]
    }
   ],
   "source": [
    "### loading data\n",
    "# Example usage\n",
    "#hdf5_file = 'partial_mdnet_40.h5'\n",
    "hdf5_file = '/home/hani/GitHub/PCR_practice/Old_notebooks/mdnet_sorted_no_corr_clean.h5'\n",
    "#hdf5_file = 'combined.h5'\n",
    "# Reading the data\n",
    "data = load_entire_dataset(hdf5_file)\n",
    "\n",
    "print('Instances', len(list(data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract source and target point clouds ###\n",
    "src_dst_data =  extract_src_dst(data)\n",
    "part_dst_data = extract_partViews_dst(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_normalize(pc):\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "def read_data_to_numpy(filepath):\n",
    "    \"\"\"\n",
    "    Reads a text file with point cloud data into a numpy array.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath: str, the path to the text file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - data: numpy.ndarray, an array of shape (n, 6) where n is the number of points.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the data points\n",
    "    data_points = []\n",
    "\n",
    "    # Open the file and read line by line\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split each line into components, convert to float, and append to the list\n",
    "            point = [float(value) for value in line.strip().split(',')]\n",
    "            data_points.append(point)\n",
    "\n",
    "    # Convert the list of data points into a numpy array\n",
    "    data = np.array(data_points)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = '/home/hani/GitHub/PCR_practice/modelnet40_normal_resampled/airplane/airplane_0015.txt'\n",
    "smaple_data_with_normals = read_data_to_numpy(sample_data_path)\n",
    "smaple_data_with_normals_ds = downsample_data_ndarray(smaple_data_with_normals,100,25)\n",
    "sample_cloud_xyz = smaple_data_with_normals_ds[:,:3]\n",
    "sample_cloud_normal = smaple_data_with_normals_ds[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_source = part_dst_data['airplane_O10_V1']\n",
    "part_dst = part_dst_data['airplane_O15_Vdst'] \n",
    "#part_dst = pc_normalize(part_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data is loaded and ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_features = estimate_robust_normals_using_open3d(B_xyz_part_1)  # Replace with actual features if available\n",
    "#additional_features = np.random.rand(part_dst.shape[0], 3)  # Replace with actual features if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_point_cloud_np = np.concatenate([B_xyz_part_1,additional_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_point_cloud_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_tensor = torch.tensor(augmented_point_cloud_np, dtype=torch.float).transpose(0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 760])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_cloud_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    prediction, l3_pts,l2_pts,l1_pts = model_pnpp_original.forward(point_cloud_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.9035, -10.1638, -10.1773,  -8.5066, -10.8212, -11.2051,  -8.8252,\n",
       "          -3.5607, -16.8487, -10.1059, -11.8154,  -4.5686, -14.8994, -12.4416,\n",
       "         -14.5933,  -6.0149, -12.5874,  -6.0080,  -1.9921,  -9.2863, -11.1617,\n",
       "         -13.3949, -11.4661, -19.5833,  -7.8436,  -8.2134,  -0.2854,  -6.6824,\n",
       "         -12.5273, -10.3016,  -9.6386,  -4.7567, -15.6452, -10.6411,  -6.7339,\n",
       "         -12.0475, -15.5110,  -7.6033, -14.5655,  -7.8309]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26])\n"
     ]
    }
   ],
   "source": [
    "_, predicted_class = torch.max(prediction, 1)  # Returns the value and index of the maximum value in each row\n",
    "# or\n",
    "predicted_class = prediction.argmax(1)  # Directly returns the index of the maximum value in each row\n",
    "\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3_pts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create partial data\n",
    "\n",
    "def fixed_viewpoints():\n",
    "    \"\"\"\n",
    "    Define 8 fixed viewpoints on the unit sphere using latitude and longitude.\n",
    "    \"\"\"\n",
    "    # This is an example. You might want to refine the distribution of these points\n",
    "    latitudes = [0, 0, 90, -90, 45, 45, -45, -45]\n",
    "    longitudes = [0, 180, 0, 0, 45, -135, 45, -135]\n",
    "\n",
    "    viewpoints = []\n",
    "    for lat, lon in zip(latitudes, longitudes):\n",
    "        lat_rad = np.radians(lat)\n",
    "        lon_rad = np.radians(lon)\n",
    "\n",
    "        x = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "        y = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "        z = np.sin(lat_rad)\n",
    "        viewpoints.append([x, y, z])\n",
    "    return np.array(viewpoints)\n",
    "\n",
    "def points_in_view_frame(A_xyz, viewpoint, frame_size=1):\n",
    "    \"\"\"\n",
    "    Select points from the point cloud that are visible within the view frame of the given viewpoint.\n",
    "\n",
    "    Parameters:\n",
    "    A_xyz (numpy.ndarray): The point cloud as an n x 3 numpy array.\n",
    "    viewpoint (numpy.ndarray): A 3D point representing the viewpoint.\n",
    "    frame_size (float): The size of the view frame. Smaller values mean a narrower field of view.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Points visible within the view frame.\n",
    "    \"\"\"\n",
    "    visible_points = []\n",
    "    for point in A_xyz:\n",
    "        # Project point onto the view direction plane\n",
    "        direction = point - viewpoint\n",
    "        distance = np.linalg.norm(direction)\n",
    "        if distance <= frame_size:\n",
    "            visible_points.append(point)\n",
    "    return np.array(visible_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple partial views ###\n",
    "viewpoints = fixed_viewpoints()\n",
    "\n",
    "# Creating 8 partial point clouds\n",
    "B_xyz_part_1 = points_in_view_frame(sample_cloud_xyz, viewpoints[0]) \n",
    "B_xyz_part_2 = points_in_view_frame(sample_cloud_xyz, viewpoints[1]) \n",
    "B_xyz_part_3 = points_in_view_frame(sample_cloud_xyz, viewpoints[2]) \n",
    "B_xyz_part_4 = points_in_view_frame(sample_cloud_xyz, viewpoints[3]) \n",
    "B_xyz_part_5 = points_in_view_frame(sample_cloud_xyz, viewpoints[4]) \n",
    "B_xyz_part_6 = points_in_view_frame(sample_cloud_xyz, viewpoints[5]) \n",
    "B_xyz_part_7 = points_in_view_frame(sample_cloud_xyz, viewpoints[6]) \n",
    "B_xyz_part_8 = points_in_view_frame(sample_cloud_xyz, viewpoints[7]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "viz1pcl_xyz(B_xyz_part_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet2pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
